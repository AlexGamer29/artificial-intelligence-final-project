{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langid.langid import LanguageIdentifier, model\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the crawled and labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_path(folder_path):\n",
    "    examples = []\n",
    "    for label in os.listdir(folder_path):\n",
    "        full_path = os.path.join (folder_path , label)\n",
    "        for file_name in os.listdir (full_path):\n",
    "            file_path = os.path . join (full_path, file_name)\n",
    "            with open (file_path, \"r\", encoding =\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "            sentence = \" \".join(lines)\n",
    "            if label == \"neg\": label = 0\n",
    "            if label == \"pos\": label = 1\n",
    "            data = {\n",
    "                'sentence': sentence ,\n",
    "                'label': label\n",
    "            }\n",
    "            examples.append(data)\n",
    "    return pd. DataFrame ( examples )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_paths = {\n",
    "    'train': './data/data_train/train',\n",
    "    'valid': './data/data_train/test',\n",
    "    'test': './data/data_test/test'\n",
    "}\n",
    "train_df = load_data_from_path(folder_paths ['train'])\n",
    "valid_df = load_data_from_path(folder_paths ['valid'])\n",
    "test_df = load_data_from_path(folder_paths ['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean all of the non-Vietnamese comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_vn (df):\n",
    "    identifier = LanguageIdentifier.from_modelstring(model , norm_probs = True )\n",
    "    not_vi_idx = set ()\n",
    "    THRESHOLD = 0.9\n",
    "\n",
    "    for idx, row in df.iterrows ():\n",
    "        score = identifier.classify(row[\"sentence\"])\n",
    "        if score[0] != \"vi\" or (score [0] == \"vi\" and score [1] <= THRESHOLD):\n",
    "            not_vi_idx.add(idx)\n",
    "    vi_df = df [~df.index.isin(not_vi_idx)]\n",
    "    not_vi_df = df[df.index.isin(not_vi_idx)]\n",
    "\n",
    "    return vi_df , not_vi_df\n",
    "train_df_vi , train_df_other = identify_vn ( train_df )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data:\n",
    "- Clean the HTML tag, URLs,...\n",
    "- Remove punctuations, numbers,...\n",
    "- Remove special characters, emoticons,...\n",
    "- Remove blank space\n",
    "- Convert to lower case\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dan\\AppData\\Local\\Temp\\ipykernel_20960\\1421259786.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df_vi ['preprocess_sentence'] = [ preprocess_text ( row['sentence']) for index, row in train_df_vi.iterrows () ]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    url_pattern = re.compile(r'https?://\\s+\\wwww\\.\\s+')\n",
    "    text = url_pattern.sub(r\" \", text)\n",
    "    html_pattern = re.compile (r'<[^<>]+>')\n",
    "    text = html_pattern.sub(\" \", text)\n",
    "\n",
    "    replace_chars = list(string.punctuation + string.digits)\n",
    "    for char in replace_chars :\n",
    "        text = text.replace(char, \" \")\n",
    "\n",
    "    emoji_pattern = re.compile (\"[\"\n",
    "        u\"\\ U0001F600 -\\ U0001F64F \" # emoticons\n",
    "        u\"\\ U0001F300 -\\ U0001F5FF \" # symbols & pictographs\n",
    "        u\"\\ U0001F680 -\\ U0001F6FF \" # transport & map symbols\n",
    "        u\"\\ U0001F1E0 -\\ U0001F1FF \" # flags (iOS)\n",
    "        u\"\\ U0001F1F2 -\\ U0001F1F4 \" # Macau flag\n",
    "        u\"\\ U0001F1E6 -\\ U0001F1FF \" # flags\n",
    "        u\"\\ U0001F600 -\\ U0001F64F \"\n",
    "        u\"\\ U00002702 -\\ U000027B0 \"\n",
    "        u\"\\ U000024C2 -\\ U0001F251 \"\n",
    "        u\"\\ U0001f926 -\\ U0001f937 \"\n",
    "        u\"\\ U0001F1F2 \"\n",
    "        u\"\\ U0001F1F4 \"\n",
    "        u\"\\ U0001F620 \"\n",
    "        u\"\\ u200d \"\n",
    "        u\"\\u2640 -\\ u2642 \"\n",
    "    \"]+\", flags =re. UNICODE )\n",
    "    text = emoji_pattern.sub(r\" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text.lower()\n",
    "\n",
    "train_df_vi ['preprocess_sentence'] = [ preprocess_text ( row['sentence']) for index, row in train_df_vi.iterrows () ]\n",
    "valid_df ['preprocess_sentence'] = [ preprocess_text ( row['sentence']) for index, row in valid_df.iterrows () ]\n",
    "test_df ['preprocess_sentence'] = [ preprocess_text ( row['sentence']) for index, row in test_df.iterrows () ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the cleaned text into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word - based tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# create iter dataset\n",
    "def yield_tokens (sentences, tokenizer):\n",
    "    for sentence in sentences :\n",
    "        yield tokenizer(sentence)\n",
    "        \n",
    "# build vocabulary\n",
    "vocab_size = 10000\n",
    "vocabulary = build_vocab_from_iterator(\n",
    "    yield_tokens(train_df_vi['preprocess_sentence'], tokenizer),\n",
    "    max_tokens = vocab_size,\n",
    "    specials =[\"<unk>\"]\n",
    ")\n",
    "\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
    "\n",
    "# convert iter into torchtext dataset\n",
    "def prepare_dataset (df):\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row['preprocess_sentence']\n",
    "        encoded_sentence = vocabulary(tokenizer(sentence))\n",
    "        label = row['label']\n",
    "        yield encoded_sentence, label\n",
    "\n",
    "train_dataset = prepare_dataset(train_df_vi)\n",
    "train_dataset = to_map_style_dataset(train_dataset)\n",
    "\n",
    "valid_dataset = prepare_dataset(valid_df)\n",
    "valid_dataset = to_map_style_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch (batch):\n",
    "    encoded_sentences, labels, offsets = [], [], [0]\n",
    "    for encoded_sentence, label in batch :\n",
    "        labels.append(label)\n",
    "        encoded_sentence = torch.tensor ( encoded_sentence, dtype = torch.int64 )\n",
    "        encoded_sentences.append (encoded_sentence)\n",
    "        offsets.append ( encoded_sentence.size(0))\n",
    "\n",
    "    labels = torch.tensor(labels , dtype = torch . int64 )\n",
    "    offsets = torch.tensor( offsets [:-1]).cumsum(dim = 0)\n",
    "    encoded_sentences = torch.cat(encoded_sentences)\n",
    "    return encoded_sentences.to(device), offsets.to(device), labels.to(device)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader (\n",
    "    train_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True,\n",
    "    collate_fn = collate_batch\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader (\n",
    "    valid_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    "    collate_fn = collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Text Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self , vocab_size , embed_dim , num_class):\n",
    "        super ( TextClassificationModel , self ). __init__ ()\n",
    "        self . embedding = nn. EmbeddingBag ( vocab_size , embed_dim , sparse = False )\n",
    "        self .fc = nn. Linear ( embed_dim , num_class )\n",
    "        self . init_weights ()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(- initrange, initrange )\n",
    "        self.fc.weight.data.uniform_(- initrange, initrange )\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward (self, inputs, offsets):\n",
    "        embedded = self . embedding (inputs , offsets )\n",
    "        return self .fc( embedded )\n",
    "\n",
    "num_class = len(set( train_df_vi ['label']))\n",
    "vocab_size = len( vocabulary )\n",
    "embed_dim = 256\n",
    "model = TextClassificationModel( vocab_size, embed_dim, num_class ).to( device )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 5\n",
    "criterion = torch.nn.CrossEntropyLoss ()\n",
    "optimizer = torch.optim.SGD( model.parameters (), lr= learning_rate )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train (model , optimizer , criterion , train_dataloader , epoch = 0, log_interval = 25) :\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    for idx , (inputs, offsets, labels ) in enumerate ( train_dataloader ):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(inputs, offsets )\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion (predictions, labels )\n",
    "        losses.append(loss.item ())\n",
    "\n",
    "        # backward\n",
    "        loss.backward ()\n",
    "        torch.nn.utils.clip_grad_norm_( model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += ( predictions . argmax (1) == labels ). sum (). item ()\n",
    "        total_count += labels . size (0)\n",
    "\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time . time () - start_time\n",
    "            print (\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \\n | accuracy {:8.3f}\".format(epoch, idx, len( train_dataloader ), total_acc / total_count)\n",
    "            )\n",
    "            total_acc , total_count = 0, 0\n",
    "            start_time = time . time ()\n",
    "\n",
    "    epoch_acc = total_acc / total_count\n",
    "    epoch_loss = sum( losses ) / len ( losses )\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate (model , criterion , valid_dataloader ):\n",
    "    model.eval ()\n",
    "    total_acc , total_count = 0, 0\n",
    "    losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx , (inputs, offsets, labels ) in enumerate ( valid_dataloader ):\n",
    "            predictions = model(inputs, offsets )\n",
    "            loss = criterion(predictions, labels )\n",
    "            losses . append(loss)\n",
    "            total_acc += (predictions.argmax(1) == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    epoch_acc = total_acc / total_count\n",
    "    epoch_loss = sum(losses) / len (losses)\n",
    "    return epoch_acc, epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_class = len(set( train_df_vi ['label']))\n",
    "vocab_size = len( vocabulary )\n",
    "embed_dim = 100\n",
    "model = TextClassificationModel( vocab_size, embed_dim, num_class ).to(device)\n",
    "\n",
    "learning_rate = 5\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    25/  233 batches \n",
      " | accuracy    0.885\n",
      "| epoch   1 |    50/  233 batches \n",
      " | accuracy    0.877\n",
      "| epoch   1 |    75/  233 batches \n",
      " | accuracy    0.887\n",
      "| epoch   1 |   100/  233 batches \n",
      " | accuracy    0.877\n",
      "| epoch   1 |   125/  233 batches \n",
      " | accuracy    0.884\n",
      "| epoch   1 |   150/  233 batches \n",
      " | accuracy    0.896\n",
      "| epoch   1 |   175/  233 batches \n",
      " | accuracy    0.880\n",
      "| epoch   1 |   200/  233 batches \n",
      " | accuracy    0.876\n",
      "| epoch   1 |   225/  233 batches \n",
      " | accuracy    0.890\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   1 | Time :  1.78s | Train Accuracy    0.876 | Train Loss    0.309 \n",
      " | Valid Accuracy    0.855 | Valid Loss    0.393 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |    25/  233 batches \n",
      " | accuracy    0.888\n",
      "| epoch   2 |    50/  233 batches \n",
      " | accuracy    0.880\n",
      "| epoch   2 |    75/  233 batches \n",
      " | accuracy    0.891\n",
      "| epoch   2 |   100/  233 batches \n",
      " | accuracy    0.880\n",
      "| epoch   2 |   125/  233 batches \n",
      " | accuracy    0.878\n",
      "| epoch   2 |   150/  233 batches \n",
      " | accuracy    0.890\n",
      "| epoch   2 |   175/  233 batches \n",
      " | accuracy    0.890\n",
      "| epoch   2 |   200/  233 batches \n",
      " | accuracy    0.886\n",
      "| epoch   2 |   225/  233 batches \n",
      " | accuracy    0.877\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   2 | Time :  1.76s | Train Accuracy    0.886 | Train Loss    0.305 \n",
      " | Valid Accuracy    0.860 | Valid Loss    0.358 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |    25/  233 batches \n",
      " | accuracy    0.881\n",
      "| epoch   3 |    50/  233 batches \n",
      " | accuracy    0.898\n",
      "| epoch   3 |    75/  233 batches \n",
      " | accuracy    0.888\n",
      "| epoch   3 |   100/  233 batches \n",
      " | accuracy    0.880\n",
      "| epoch   3 |   125/  233 batches \n",
      " | accuracy    0.888\n",
      "| epoch   3 |   150/  233 batches \n",
      " | accuracy    0.881\n",
      "| epoch   3 |   175/  233 batches \n",
      " | accuracy    0.888\n",
      "| epoch   3 |   200/  233 batches \n",
      " | accuracy    0.890\n",
      "| epoch   3 |   225/  233 batches \n",
      " | accuracy    0.876\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   3 | Time :  1.78s | Train Accuracy    0.897 | Train Loss    0.299 \n",
      " | Valid Accuracy    0.873 | Valid Loss    0.352 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |    25/  233 batches \n",
      " | accuracy    0.889\n",
      "| epoch   4 |    50/  233 batches \n",
      " | accuracy    0.886\n",
      "| epoch   4 |    75/  233 batches \n",
      " | accuracy    0.892\n",
      "| epoch   4 |   100/  233 batches \n",
      " | accuracy    0.875\n",
      "| epoch   4 |   125/  233 batches \n",
      " | accuracy    0.895\n",
      "| epoch   4 |   150/  233 batches \n",
      " | accuracy    0.888\n",
      "| epoch   4 |   175/  233 batches \n",
      " | accuracy    0.878\n",
      "| epoch   4 |   200/  233 batches \n",
      " | accuracy    0.885\n",
      "| epoch   4 |   225/  233 batches \n",
      " | accuracy    0.882\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   4 | Time :  1.77s | Train Accuracy    0.877 | Train Loss    0.299 \n",
      " | Valid Accuracy    0.866 | Valid Loss    0.357 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |    25/  233 batches \n",
      " | accuracy    0.887\n",
      "| epoch   5 |    50/  233 batches \n",
      " | accuracy    0.878\n",
      "| epoch   5 |    75/  233 batches \n",
      " | accuracy    0.890\n",
      "| epoch   5 |   100/  233 batches \n",
      " | accuracy    0.877\n",
      "| epoch   5 |   125/  233 batches \n",
      " | accuracy    0.879\n",
      "| epoch   5 |   150/  233 batches \n",
      " | accuracy    0.889\n",
      "| epoch   5 |   175/  233 batches \n",
      " | accuracy    0.885\n",
      "| epoch   5 |   200/  233 batches \n",
      " | accuracy    0.891\n",
      "| epoch   5 |   225/  233 batches \n",
      " | accuracy    0.887\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   5 | Time :  1.72s | Train Accuracy    0.907 | Train Loss    0.294 \n",
      " | Valid Accuracy    0.847 | Valid Loss    0.378 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |    25/  233 batches \n",
      " | accuracy    0.888\n",
      "| epoch   6 |    50/  233 batches \n",
      " | accuracy    0.893\n",
      "| epoch   6 |    75/  233 batches \n",
      " | accuracy    0.892\n",
      "| epoch   6 |   100/  233 batches \n",
      " | accuracy    0.888\n",
      "| epoch   6 |   125/  233 batches \n",
      " | accuracy    0.885\n",
      "| epoch   6 |   150/  233 batches \n",
      " | accuracy    0.892\n",
      "| epoch   6 |   175/  233 batches \n",
      " | accuracy    0.893\n",
      "| epoch   6 |   200/  233 batches \n",
      " | accuracy    0.889\n",
      "| epoch   6 |   225/  233 batches \n",
      " | accuracy    0.886\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   6 | Time :  1.73s | Train Accuracy    0.874 | Train Loss    0.293 \n",
      " | Valid Accuracy    0.869 | Valid Loss    0.356 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |    25/  233 batches \n",
      " | accuracy    0.888\n",
      "| epoch   7 |    50/  233 batches \n",
      " | accuracy    0.892\n",
      "| epoch   7 |    75/  233 batches \n",
      " | accuracy    0.891\n",
      "| epoch   7 |   100/  233 batches \n",
      " | accuracy    0.888\n",
      "| epoch   7 |   125/  233 batches \n",
      " | accuracy    0.885\n",
      "| epoch   7 |   150/  233 batches \n",
      " | accuracy    0.885\n",
      "| epoch   7 |   175/  233 batches \n",
      " | accuracy    0.898\n",
      "| epoch   7 |   200/  233 batches \n",
      " | accuracy    0.882\n",
      "| epoch   7 |   225/  233 batches \n",
      " | accuracy    0.893\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   7 | Time :  1.72s | Train Accuracy    0.892 | Train Loss    0.290 \n",
      " | Valid Accuracy    0.871 | Valid Loss    0.364 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |    25/  233 batches \n",
      " | accuracy    0.892\n",
      "| epoch   8 |    50/  233 batches \n",
      " | accuracy    0.890\n",
      "| epoch   8 |    75/  233 batches \n",
      " | accuracy    0.888\n",
      "| epoch   8 |   100/  233 batches \n",
      " | accuracy    0.887\n",
      "| epoch   8 |   125/  233 batches \n",
      " | accuracy    0.887\n",
      "| epoch   8 |   150/  233 batches \n",
      " | accuracy    0.900\n",
      "| epoch   8 |   175/  233 batches \n",
      " | accuracy    0.881\n",
      "| epoch   8 |   200/  233 batches \n",
      " | accuracy    0.885\n",
      "| epoch   8 |   225/  233 batches \n",
      " | accuracy    0.906\n",
      "-----------------------------------------------------------\n",
      "| End of epoch   8 | Time :  1.70s | Train Accuracy    0.892 | Train Loss    0.288 \n",
      " | Valid Accuracy    0.871 | Valid Loss    0.358 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 6\n",
    "for epoch in range(1, num_epochs +1):\n",
    "    epoch_start_time = time.time()\n",
    "    train_acc, train_loss = train(model, optimizer, criterion, train_dataloader, epoch)\n",
    "    eval_acc, eval_loss = evaluate(model, criterion, valid_dataloader)\n",
    "    print (\"-\" * 59)\n",
    "    print (\n",
    "        \"| End of epoch {:3d} | Time : {:5.2f}s | Train Accuracy {:8.3f} | Train Loss {:8.3f} \\n | Valid Accuracy {:8.3f} | Valid Loss {:8.3f} \"\n",
    "        .format(epoch, time.time() - epoch_start_time, train_acc , train_loss , eval_acc , eval_loss)\n",
    "    )\n",
    "    print (\"-\" * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict ( text ):\n",
    "    with torch.no_grad():\n",
    "        encoded = torch.tensor(vocabulary(tokenizer(text)))\n",
    "        output = model(encoded, torch.tensor([0]))\n",
    "    return output.argmax(1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8717)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels = [], []\n",
    "for index, row in test_df.iterrows ():\n",
    "    sentence = row['preprocess_sentence']\n",
    "    label = row['label']\n",
    "    prediction = predict( sentence )\n",
    "    predictions.append( prediction )\n",
    "    labels.append( label )\n",
    "\n",
    "sum (torch.tensor(predictions) == torch.tensor( labels ))/len(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
